# -*- coding: utf-8 -*-
"""Lab 2 ANLY 535

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ee1_yaiYEqpAgzb25mhm9OXcZ-y487B
"""

from tensorflow.python.client import device_lib

print("Show System RAM Memory:\n\n")
!cat /proc/meminfo | egrep "MemTotal*"

print("\n\nShow Devices:\n\n"+str(device_lib.list_local_devices()))

import tensorflow as tf
tf.version

from google.colab import drive
drive.mount('/content/gdrive')

import time
start=time.time()
!python "/mnist_cnn.py"
end=time.time()
print(end-start)

#Question 1: Google Colab run time for MNIST CNN code was 77.16 seconds vs 537.90 seconds on my computer.

from tensorflow.keras.datasets import mnist 

 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Dropout, Flatten 
from tensorflow.keras.layers import Conv2D, MaxPooling2D 
from tensorflow.keras import backend as K 
 
batch_size = 128 
num_classes = 10 
epochs = 12

# input image dimensions 
img_rows, img_cols = 28, 28 
 
# the data, split between train and test sets 
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':     
  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)     
  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)     
  input_shape = (1, img_rows, img_cols) 
else:     
  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)     
  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)     
  input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32') 
x_test = x_test.astype('float32') 
x_train /= 255 
x_test /= 255 
print('x_train shape:', x_train.shape) 
print(x_train.shape[0], 'train samples') 
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices 
import keras
y_train = keras.utils.to_categorical(y_train, num_classes) 
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential() 
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', 
input_shape=input_shape)) 
model.add(Conv2D(64, (3, 3), activation='relu')) 
model.add(MaxPooling2D(pool_size=(2, 2))) 
model.add(Dropout(0.25)) 
model.add(Flatten()) 
model.add(Dense(128, activation='relu')) 
model.add(Dropout(0.5)) 
model.add(Dense(num_classes, activation='softmax'))

#Question 2: Explain the way the model is designed. Talk about all the layers
#and their functionality. This is a sequential model which groups a linear plain stack of layers where
#each stack has exactly one input tensor and exactly one output tensor. A Conv2D layer creates a 
#convolution kernel that is convolved with the layer input to produce a tensor of outputs. 
#Kernel size is an integer or tuple/list of 2 integers, specifying the height and width of the 2D 
#convolution window. Can be a single integer to specify the same value for all spatial dimensions. 
#Applies the rectified linear unit activation function. With default values, this returns the 
#standard ReLU activation: max(x, 0) , the element-wise maximum of 0 and the input tensor.
#MaxPooling 2D is a pooling or downsampling function that reduces the dimensionality of each feature map
#but retains the most important information; max pooling returns the maximum value in the pool.
#Here max pooling is with 2x2 filters. In the dropout layer randomly removes some nodes in the 
#incoming and outgoing connections; in droput, we identify which neursons we want to keep.
#The Flatten layer converts a matrix into a single array. A dense layer represents a matrix vector
#multiplication.
# The last output layer uses a softmax activation whic is 
#used when you have more than two classes; it is a variation of the sigmoid function.

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])                
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_split=0.2)            
score = model.evaluate(x_test, y_test, verbose=0) 
print('Test loss:', score[0]) 
print('Test accuracy:', score[1])

import matplotlib.pyplot as plt
plt.subplot(2,1,1) 
plt.plot(hist.history['accuracy']) 
plt.plot(hist.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='lower right') 
 
plt.subplot(2,1,2) 
plt.plot(hist.history['loss']) 
plt.plot(hist.history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper right') 
 
plt.show() 
#Question 3: The accuracy is 99.2%. The training accuracy is above the testing accuracy.
#The training loss is way below the testing loss. This means that this model is overfitting.

import pandas as pd

df = pd.read_csv('amazon_cells_labelled.txt', names=[ 'sentence', 'label'], sep='\t')

print(df.iloc[0])

sentences = ['John likes ice cream', 'John hates chocolate.'] 
from sklearn.feature_extraction.text import CountVectorizer 
vectorizer = CountVectorizer(min_df=0, lowercase=False) 
vectorizer.fit(sentences) 
vectorizer.vocabulary_

vectorizer.transform(sentences).toarray()

from sklearn.model_selection import train_test_split 
sentences = df['sentence'].values 
y = df['label'].values 
sentences_train, sentences_test, y_train, y_test = train_test_split(
 sentences, y, test_size=0.25, random_state=1000)

from sklearn.feature_extraction.text import CountVectorizer 
 
vectorizer = CountVectorizer() 
vectorizer.fit(sentences_train) 
 
X_train = vectorizer.transform(sentences_train).toarray()
X_test  = vectorizer.transform(sentences_test).toarray()
X_train

from sklearn.linear_model import LogisticRegression 
 
classifier = LogisticRegression() 
classifier.fit(X_train, y_train) 
score = classifier.score(X_test, y_test) 
 
print("Accuracy:", score)

from tensorflow.keras.models import Sequential 
from tensorflow.keras import layers 
 
input_dim = X_train.shape[1]  # Number of features 
 
model = Sequential() 
model.add(layers.Dense(10, input_dim=input_dim, activation='relu')) 
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
model.summary()

hist=model.fit(X_train, y_train,
                 epochs=100, 
                 validation_split=0.2,
                 batch_size=10) 
loss, accuracy = model.evaluate(X_test, y_test, verbose=False) 
print("Test Accuracy: ",accuracy*100)

import matplotlib.pyplot as plt
plt.subplot(2,1,1) 
plt.plot(hist.history['accuracy']) 
plt.plot(hist.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='lower right') 
 
plt.subplot(2,1,2) 
plt.plot(hist.history['loss']) 
plt.plot(hist.history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper right') 
 
plt.show()

#Question 4: Explain the graphs above-The training accuracy is way above the testing accuracy while
#the training loss is way below the testing loss. Thus, this model is overfitting. A dropout layer can 
#be added to prevent overfitting.

#Make the model by creating 3 hidden layers (first one 200 nodes, second one 100 nodes and last one 
#50 nodes and after each step, 
#add dropout of 0.2 and report the accuracy. The dropout below barely increased to 78.8%

from tensorflow.keras.models import Sequential 
from tensorflow.keras import layers 
 
input_dim = X_train.shape[1]  # Number of features 
 
model = Sequential() 
model.add(layers.Dense(200, input_dim=input_dim, activation='relu')) 
model.add(layers.Dropout(0.2))
model.add(layers.Dense(100, activation='sigmoid')) 
model.add(layers.Dropout(0.2))
model.add(layers.Dense(50, activation='sigmoid'))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 
model.summary()

hist=model.fit(X_train, y_train,
                 epochs=100, 
                 validation_split=0.2,
                 batch_size=10) 
loss, accuracy = model.evaluate(X_test, y_test, verbose=False) 
print("Test Accuracy: ",accuracy*100)

from keras.preprocessing.text import Tokenizer 
 
tokenizer = Tokenizer(num_words=5000) 
tokenizer.fit_on_texts(sentences_train) 
 
X_train = tokenizer.texts_to_sequences(sentences_train) 
X_test = tokenizer.texts_to_sequences(sentences_test) 
 
vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index 
 
print(sentences_train[3]) 
print(X_train[3])

for word in ['the', 'all', 'happy']:      
  print('{}: {}'.format(word, tokenizer.word_index[word]))

from keras.preprocessing.sequence import pad_sequences 
 
maxlen = 100 

# Pad variables with zeros  
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) 
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen) 
print(X_train[0, :])

from tensorflow.keras.models import Sequential 
from tensorflow.keras import layers 
 
embedding_dim = 50 
 
model = Sequential() 
model.add(layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=maxlen)) 
model.add(layers.GlobalMaxPool1D()) 
model.add(layers.Dense(10, activation='relu')) 
model.add(layers.Dense(1, activation='sigmoid')) 
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) 
model.summary()

hist = model.fit(X_train, y_train, epochs=50, validation_split=0.2, batch_size=10) 
loss, accuracy = model.evaluate(X_test, y_test, verbose=False) 
print("Accuracy: ",accuracy)

plt.subplot(2,1,1) 
plt.plot(hist.history['accuracy']) 
plt.plot(hist.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='lower right') 
 
plt.subplot(2,1,2) 
plt.plot(hist.history['loss']) 
plt.plot(hist.history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper right') 
 
plt.show()

#Question 5:How do you interpret the results? The training data is way above the testing 
#model accuracy and the test loss is way above the training loss showing overfitting in the model.
#Question 6: What is your recommendation
#to improve the accuracy? Implement your idea. Adding a droput layer would improve the accuracy in theory.
#However, after applying the Dropout layer below, it barely improved the accuracy up to 82.4%

for word in ['the', 'all', 'happy']:      
  print('{}: {}'.format(word, tokenizer.word_index[word]))

from keras.preprocessing.text import Tokenizer 
 
tokenizer = Tokenizer(num_words=5000) 
tokenizer.fit_on_texts(sentences_train) 
 
X_train = tokenizer.texts_to_sequences(sentences_train) 
X_test = tokenizer.texts_to_sequences(sentences_test) 
 
vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index 
 
print(sentences_train[3]) 
print(X_train[3])

from keras.preprocessing.sequence import pad_sequences 
 
maxlen = 100 

# Pad variables with zeros  
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) 
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen) 
print(X_train[0, :])

from tensorflow.keras.models import Sequential 
from tensorflow.keras import layers 

 
embedding_dim = 50 
 
model = Sequential() 
model.add(layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=maxlen)) 
model.add(layers.GlobalMaxPool1D()) 
model.add(layers.Dense(10, activation='relu'))
model.add(layers.Dropout(0.2)) 
model.add(layers.Dense(1, activation='sigmoid')) 

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) 
model.summary()

hist = model.fit(X_train, y_train, epochs=50, validation_split=0.2, batch_size=10) 
loss, accuracy = model.evaluate(X_test, y_test, verbose=False) 
print("Accuracy: ",accuracy)

plt.subplot(2,1,1) 
plt.plot(hist.history['accuracy']) 
plt.plot(hist.history['val_accuracy']) 
plt.title('model accuracy') 
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='lower right') 
 
plt.subplot(2,1,2) 
plt.plot(hist.history['loss']) 
plt.plot(hist.history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper right') 
 
plt.show()