---
title: 'Factor Analysis'
author: "Roger Geertz Gonzalez"
date: "June 23, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries + functions

Load all the libraries or functions that you will use to for the rest of the assignment. It is helpful to define your libraries and functions at the top of a report, so that others can know what they need for the report to compile correctly.

```{r libaries}
##r chunk
library(Rling)
library(psych)
library(reticulate)
```

Do the same for the Python libraries you will need. 


## The Data

The data is provided as `liwc_house_conflict.csv`. We collected over 1000 different speeches given on the floor of the US House of Representatives that discussed different war time conflicts with Iraq, Kuwait, Russia, Syria, Iran, and a few others. This data was then processed with the Linguistic Inquiry and Word Count software, which provides a linguistic frequency analysis for many categories. 

You should pick 15-20 categories that you think might cluster together and/or be interesting to examine for their register relatedness. You can learn more about the categories by checking out the attached manual starting on page four. Do not use the "total" categories with their subgroups or you might get a singular matrix error. You might also consider running a quick summary on your choosen categories as well, to make sure they are not effectly zero frequency (i.e., most of the informal language ones will be very small percents due to the location of the speech).

Import your data and create a data frame here with only the categories you are interested in.

```{r thedata}
##r chunk
library(dplyr)
df <- read.csv("liwc_house_conflict.csv")
rownames(df) <- df[,1]
dFrame <- df %>% dplyr::select(Authentic,Tone,WPS,Sixltr,Dic,posemo,negemo,female,male,see,hear,feel,work,leisure,home) 

data(dFrame)
head(dFrame)
```

Transfer the data over to python to use as well. 

```{python}
##python chunk
#open library
import pandas as pd

#move over data from R or import it with pd.read_csv
dFrame_py = r.dFrame

#look at the data
dFrame_py.head()


```

## Before you start

Include Bartlett's test and the KMO statistic to determine if you have adequate correlations and sampling before running an EFA. 

```{r beforeyougo}
##r chunk
correlations = cor(dFrame[ , -1 ])
cortest.bartlett(correlations, n = nrow(dFrame)) 

KMO(correlations)

py_install("factor_analyzer", pip=T)

## $chisq
```

Include Bartlett's test and the KMO statistic from Python. Do they appear to match? 

```{python}
##python chunk
#import bartlett test
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity

#calculate bartlett
chi_square_value, p_value = calculate_bartlett_sphericity(dFrame_py)

#output the answer
chi_square_value, p_value

from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(dFrame_py)

kmo_all

kmo_model

```

## How many factors?

- Explore how many factors you should use.
  - Include a parallel analysis and scree plot.
  - Sum the Kaiser criterion.
  - Go with the smaller number of items or the most agreement between different criteria. 

```{r howmany}
##r chunk
number_items = fa.parallel(dFrame[, -1], ##dataset
                           fm = "ml", ##type of math
                           fa = "both") #look at both efa/pca
```

- Include the scree plot and summation of the eigenvalues from Python. 

```{python}
##python chunk
#save factor analysis function
from factor_analyzer import FactorAnalyzer
fa = FactorAnalyzer(n_factors = len(dFrame_py.columns),
                    rotation = None)

#run an analysis just to get the eigenvalues
fa.fit(dFrame_py)

#view the eigenvalues

ev, v = fa.get_eigenvalues()
ev
```

## Simple structure - run the EFA

- Run the EFA in both R and Python
  - Include the saved `fa` code, but then be sure to print out the results, so the summary is on your report.
  - Plot the results from your analysis. 

```{r runit}
##r chunk
##save it

##print it out
EFA_fit = fa(dFrame[, -1], #data
             nfactors = 4, #number of factors
             rotate = "oblimin", #rotation
             fm = "ml") #math

EFA_fit$loadings #look at the full results

EFA_fit

##plot the results
fa.plot(EFA_fit, 
     labels = colnames(dFrame[ , -1]))

fa.diagram(EFA_fit)
```

- For Python, run the factor analysis and print out the loadings. Do they appear to have the same results?

```{python}
##python chunk
fa = FactorAnalyzer(n_factors = 4, rotation = "oblimin")
fa.fit(dFrame_py)

fa.loadings_ #notice underscore 

fa.get_factor_variance() ##ss, prop, cumulative
```

## Adequate solution

- Examine the fit indice(s). Are they any good? How might you interpret them?
- Examine the results - what do they appear to tell you? Are there groupings of variables in these analyses that might explain different structures/jargons/registers in language we find in Congress? 

```{r}
EFA_fit$rms #Root mean square of the residuals

EFA_fit$RMSEA #root mean squared error of approximation

EFA_fit$TLI #tucker lewis index
```

