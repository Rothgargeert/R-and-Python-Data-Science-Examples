{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras) (1.16.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras) (1.3.1)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/34/d5/ce8c17971067c0184c9045112b755be5461d5ce5253ef65a367e1298d7c5/tensorflow-2.1.0-cp37-cp37m-win_amd64.whl (355.8MB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
      "Collecting protobuf>=3.8.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/30/1b7ccde09bf0c535d11f18a574ed7d7572c729a8f754fd568b297be08b61/protobuf-3.11.3-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.5)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\" (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/51/046cbc61c7607e5ecead6ff1a9453fba5e7e47a5ea8d608cc7036586a5ef/scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9MB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/04/3c/ed9d6f653422777d11c8df359b57e569f630264fbc19962819c5ef2cc056/grpcio-1.27.1-cp37-cp37m-win_amd64.whl (1.9MB)\n",
      "Collecting gast==0.2.2 (from tensorflow)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d5/16/c5a68ef8c62406b3bbd8f49199bbae56feb390746a284c4cf036c687465f/Markdown-3.2-py2.py3-none-any.whl (88kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (41.4.0)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rothg\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.9.11)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: absl-py\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-cp37-none-any.whl size=121936 sha256=c9bd6bfde0b661389631dfc2a19251095955e935fc6335026c7d1f3ebc1bccb6\n",
      "  Stored in directory: C:\\Users\\rothg\\AppData\\Local\\pip\\Cache\\wheels\\8e\\28\\49\\fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "Successfully built absl-py\n",
      "Installing collected packages: markdown, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, grpcio, protobuf, tensorboard, astor, termcolor, google-pasta, scipy, gast, tensorflow-estimator, opt-einsum, tensorflow\n",
      "  Found existing installation: scipy 1.3.1\n",
      "    Uninstalling scipy-1.3.1:\n",
      "      Successfully uninstalled scipy-1.3.1\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 gast-0.2.2 google-auth-1.11.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.27.1 markdown-3.2 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes; binary classification problem for good or bad sentiment\n",
    "#in movie review\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length.Looking a box and whisker plot for the review lengths \n",
    "#in words, we can probably see an exponential distribution that \n",
    "#we can probably cover the mass of the distribution with a \n",
    "#clipped length of 400 to 500 words.\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\keras\\datasets\\imdb.py:49: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "         list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object), array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 1668, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 2, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 2, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 2, 2, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 2, 185, 132, 1988, 2, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 2, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 2, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 2, 7, 4, 498, 2, 748, 63, 29, 2, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 2, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 2, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 2, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 2, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 2, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 2, 2, 168, 1239, 2, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 2, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 2, 2, 3292, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 2, 2, 4, 4770, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 2, 5, 2, 68, 1830, 19, 2, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 2, 2, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 2, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 2, 7, 15, 2, 2, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 2, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 2, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 2, 9, 133, 1810, 11, 2, 2, 21, 4, 2, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 2, 3406, 718, 2, 9, 6, 2, 17, 210, 5, 3281, 2, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 1441, 2, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(nb_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import sequence\n",
    "from keras.layers.embeddings import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x2b6a38b5cc8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding(5000, 32, input_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Multi-Layer Perceptron Model for the IMDB Dataset\n",
    "# MLP for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest; 50/50 split, good standard\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bound reviews at 500 words, truncating longer reviews and zero-padding shorter reviews\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model, Embedding layer as input layer setting vocabulary to 5000,\n",
    "#word vector to 32 dimensions and input length to 500. Output is 32 x 500.\n",
    "#We will flatten the Embedded layers output to one dimension, then use \n",
    "#one dense hidden layer of 250 units with a rectifier activation function. \n",
    "#The output layer has one neuron and will use a sigmoid activation to \n",
    "#output values of 0 and 1 as predictions.The model uses logarithmic loss \n",
    "#and is optimized using the efficient ADAM optimization procedure\n",
    "\n",
    "The model uses logarithmic loss and is optimized using the efficient ADAM optimization procedure.\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 14s - loss: 0.4855 - accuracy: 0.7407 - val_loss: 0.2896 - val_accuracy: 0.8770\n",
      "Epoch 2/2\n",
      " - 13s - loss: 0.1851 - accuracy: 0.9287 - val_loss: 0.3037 - val_accuracy: 0.8764\n",
      "Accuracy: 87.64%\n"
     ]
    }
   ],
   "source": [
    "#We can fit the model and use the test set as validation while training. \n",
    "#This model overfits very quickly so we will use very few training epochs, \n",
    "#in this case just 2.There is a lot of data so we will use a batch size of 128. \n",
    "#After the model is trained, we evaluate its accuracy on the test dataset.\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Dimensional CNN for IMBD dataset\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping the data as previously in MLP above\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model, This time, after the Embedding input layer, we insert a Conv1D layer. \n",
    "#This convolutional layer has 32 feature maps and \n",
    "#reads embedded word representations 3 vector elements of the word embedding at a time.\n",
    "#The convolutional layer is followed by a 1D max pooling layer with a length \n",
    "#and stride of 2 that halves the size of the feature maps \n",
    "#from the convolutional layer. The rest of the network is the same \n",
    "#as the neural network above\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 17s - loss: 0.4446 - accuracy: 0.7664 - val_loss: 0.2733 - val_accuracy: 0.8868\n",
      "Epoch 2/2\n",
      " - 16s - loss: 0.2108 - accuracy: 0.9177 - val_loss: 0.2690 - val_accuracy: 0.8869\n",
      "Accuracy: 88.69%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model as before\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
