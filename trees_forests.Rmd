---   
title: 'Trees and Forests Assignment'
author: "Roger Geertz Gonzalez"
date: "`r Sys.Date(5/22/20)`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the libraries + functions

Load all the libraries or functions that you will use to for the rest of the assignment. It is helpful to define your libraries and functions at the top of a report, so that others can know what they need for the report to compile correctly.

The data for this project has already been loaded. You will be distinguishing between the categories of *nerd* and *geek* to determine the influence of respective variables on their category definition. 

If you are having trouble with the `Rling` library - the nerd data is avaliable on Canvas, and you can load it directly. 

```{r libraries}
##r chunk
library(Rling)
library(party)
data(nerd)
head(nerd)
```

## Description of the data

Dependent variable: 

- Noun: which category is represented either *nerd* or *geek*.

Independent variables:

- Num: a measure of social group, either pl (plural) or sg (single)
- Century: time measurement, as XX (20th) or XXI (21st) century
- Register: information about where the data was coded from ACAD (academic), MAG (magazine), NEWS (newspapers), and SPOK (spoken)
- Eval: A measure of the semanticity of the word, Neg for negative, Neutral, and Positive

## Conditional inference model

- Add a random number generator to start the model.
- Use `ctree()` to create a conditional inference model. 

```{r cimodel}
##r chunk

set.seed(12345)
tree.output = ctree(Noun ~ Num + Century + Register + Eval, data = nerd)
```

## Make a plot

- Plot the conditional inference model. 

```{r ciplot}
##r chunk
plot(tree.output)
```

## Interpret the categories 

- Write out an interpretation of the results from the model. You can interpret the branches of the tree to determine what featurally defines each category.
- With only two categories, you will see the proportion split as the output in the bar graph - look for the group with the larger proportion. 


## Conditional inference model predictiveness

- Calculate the percent correct classification for the conditional inference model. 

```{r cicorrect}
##r chunk
#Answer: The first split is on the positive or negative/neutral connotations of the #words "nerd" and "geek." The second split is on the semanticity or meaning of these #terms within the specific XXI'st or XXth Centuries.On the left side we see the #positive connotation of each of these words within these centuries. On the right #side we see the the negative/neutral connations for each of the centuries.
#On the left positive node, geek is more likely to be seen as positive in the XXI'st #Century while in the XXth Century, Geek and Nerd are likely to be seen as equally #positive. On the right side, Geek and Nerd are equally used as negative/neutral in #the XXI'st Century while on the right, Nerd is more likely to be used in a #negative/neutral way in the XXth Century.The group with the larger proportion is #Node 3 with Geek being used more positively in the XXI'st Century.
outcomes = table(predict(tree.output), nerd$Noun)
outcomes
sum(diag(outcomes)) / sum(outcomes) * 100
sum(outcomes[1]) / sum(outcomes[,1]) * 100
sum(outcomes[4]) / sum(outcomes[,2]) * 100
sum(outcomes[,1]) / (sum(outcomes[,1]) + sum(outcomes[,2]))
sum(outcomes[1,]) / (sum(outcomes[1,]) + sum(outcomes[2,]))
```

## Random forests

- Create a random forest of the same model for geek versus nerd. 

```{r forestmodel}
##r chunk
forest.output = cforest(Noun ~ Num + Century + Register + Eval, 
                        data = nerd,
                        controls = cforest_unbiased(ntree = 1000,
                                                    mtry = 2))
```

## Variable importance

- Calculate the variable importance from the random forest model.
- Include a dot plot of the importance values. 
- Which variables were the most important?

```{r forestimportance}
##r chunk
forest.importance = varimp(forest.output,
                           conditional = T)
round(forest.importance, 3)

dotchart(sort(forest.importance),
         main = "Conditional Importance of Variables")
#The most important variables are Century and Eval according to the plot below
```

## Forest model predictiveness

- Include the percent correct for the random forest model. 
- Did it do better than the conditional inference tree?

```{r forestpredict}
##r chunk
forest.outcomes = table(predict(forest.output), nerd$Noun)
forest.outcomes

sum(diag(forest.outcomes)) / sum(forest.outcomes) * 100
sum(forest.outcomes[1]) / sum(forest.outcomes[,1]) * 100
sum(forest.outcomes[4]) / sum(forest.outcomes[,2]) * 100
sum(forest.outcomes[1,]) / (sum(forest.outcomes[1,]) + sum(forest.outcomes[2,]))
#The random forest has a slightly better prediction rate than the inference tree at 
#63.37%
```

## Python model

- In this section, import the data from R to Python.
- Be sure to convert the categorical data into dummy coded data. 

```{python data_import}

import pandas as pd
Xvars = pd.get_dummies(r.nerd[["Num", "Century", "Register", "Eval"]])
Xvars.head()
```

## Create the Tree

- Create a decision tree classification of the `nerd` data. 

```{python decision_tree}
##python chunk
Yvar = pd.get_dummies(r.nerd["Noun"])
Yvar.head()

import sklearn
from sklearn import tree

CIT = tree.DecisionTreeClassifier()
CIT = CIT.fit(Xvars,Yvar)
CIT.predict(Xvars)
```

## Printing out the Tree

- Print out a text version of the classification tree. 

```{python class_tree}
##python chunk
print(tree.export_text(CIT,feature_names= list(Xvars.columns)))
```

## Confusion Matrix

```{python confusion_matrix}
##python chunk
Y_predict = pd.DataFrame(CIT.predict(Xvars))
Y_predict.columns = list(Yvar.columns)
Y_predict_category =  Y_predict.idxmax(axis=1)
Yvar_category = Yvar.idxmax(axis=1)

sklearn.metrics.confusion_matrix(Y_predict_category, Yvar_category, labels = ["nerd","geek"])



```

## Thought questions

- Are the models easier to create using R or Python (your own thoughts, they can be different than what I said in the lecture)? For me R is easier for general statistical models or at least the coding is. Python is easier for me for Machine Learning type models. Additionally, ggplot for simple plots in R is better for general statisitcs, but seaborn in Python is great for using a variety of colors.
- Which model gave you a better classification of the categories? Random forest had a better predication at 63.37%
- What other variables might be useful in understanding the category membership of geek versus nerd? Basically, what could we add to the model to improve it (there's no one right answer here - it's helpful to think about what other facets we have not measured)? More context is better when analyzing specific words and thus, one additional context that can be added to the model is to determine when geek and nerd are each used, for example: are they used in newspapers, if so which newspapers?; how are each used in school textbooks?; etc.
  